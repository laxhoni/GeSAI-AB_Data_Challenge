{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f298031a",
   "metadata": {},
   "source": [
    "# 1. Entrenamiento y Evaluación del Modelo LSTM\n",
    "\n",
    "Este notebook toma las secuencias preparadas (sin escalar) del Notebook 2 y realiza el proceso completo de entrenamiento y evaluación del modelo de predicción de fugas.\n",
    "\n",
    "**Pasos Realizados:**\n",
    "\n",
    "1.  **Carga de Datos:** Se cargan los arrays `X_sequences_unscaled.npy` (features) e `y_targets.npy` (objetivo).\n",
    "2.  **División de Datos (Train/Validation/Test):**\n",
    "    * Los datos se dividen en tres conjuntos: **entrenamiento** (60%), **validación** (20%) y **prueba** (20%).\n",
    "    * Se utiliza `stratify=y` para asegurar que la proporción de fugas (la clase minoritaria) sea similar en los tres conjuntos, lo cual es crucial para datos desbalanceados.\n",
    "3.  **Escalado de Features (`MinMaxScaler`):**\n",
    "    * Se inicializa un escalador `MinMaxScaler` para normalizar las *features* entre 0 y 1 (beneficioso para LSTMs).\n",
    "    * **Importante:** El escalador se **ajusta (`fit`) únicamente** con los datos de **entrenamiento** (`X_train`) para evitar fuga de información (*data leakage*) desde los conjuntos de validación o prueba.\n",
    "    * Luego, se **transforman** los tres conjuntos (`X_train`, `X_val`, `X_test`) usando el escalador ya ajustado.\n",
    "    * Los datos se reestructuran temporalmente a 2D para el escalado y luego se devuelven a su formato 3D original (`[muestras, pasos_de_tiempo, features]`).\n",
    "4.  **Definición de la Arquitectura del Modelo:**\n",
    "    * Se define un modelo secuencial simple usando Keras:\n",
    "        * **Entrada:** Secuencias de `N_TIMESTEPS` (ej. 72) pasos con `N_FEATURES` (ej. 9) características cada uno.\n",
    "        * **Capa LSTM:** Una capa LSTM con 64 unidades procesa la secuencia temporal. `return_sequences=False` indica que solo se necesita la salida del último paso de tiempo.\n",
    "        * **Dropout:** Se aplica Dropout (30%) para regularización y prevenir el sobreajuste.\n",
    "        * **Capas Densas:** Capas densas (`Dense`) procesan la salida del LSTM, con una capa final de 1 neurona y activación `sigmoid` para la predicción de probabilidad (0 a 1) de fuga.\n",
    "5.  **Manejo del Desbalance de Clases:**\n",
    "    * Se calculan **pesos de clase (`class_weight`)** usando la estrategia `'balanced'`. Esto asigna automáticamente un peso mayor a las muestras de la clase minoritaria (fugas) durante el entrenamiento, obligando al modelo a prestarles más atención.\n",
    "6.  **Compilación del Modelo:**\n",
    "    * Se configura el modelo para el entrenamiento con:\n",
    "        * **Optimizador:** `Adam` (un optimizador estándar y eficiente).\n",
    "        * **Función de Pérdida:** `binary_crossentropy` (adecuada para clasificación binaria).\n",
    "        * **Métricas:** Se monitorizan `AUC-PR` (Área Bajo la Curva Precisión-Recall, la métrica **más importante** para datos desbalanceados), `AUC-ROC` y `accuracy`.\n",
    "7.  **Entrenamiento del Modelo:**\n",
    "    * Se entrena el modelo (`model.fit`) usando los datos de entrenamiento escalados (`X_train_scaled`, `y_train`).\n",
    "    * Se utilizan los datos de validación escalados (`X_val_scaled`, `y_val`) para monitorizar el rendimiento en datos no vistos durante el entrenamiento.\n",
    "    * Se aplican los `class_weight` calculados.\n",
    "    * Se usa **`EarlyStopping`**: El entrenamiento se detiene automáticamente si la métrica `val_auc_pr` (AUC-PR en validación) no mejora durante un número determinado de épocas (`patience=5`), restaurando los pesos del modelo correspondientes a la mejor época.\n",
    "8.  **Evaluación del Modelo:**\n",
    "    * Se evalúa el rendimiento final del mejor modelo (restaurado por `EarlyStopping`) sobre el conjunto de **prueba escalado** (`X_test_scaled`, `y_test`), que el modelo nunca ha visto.\n",
    "    * Se imprimen las métricas clave (Loss, AUC-PR, AUC-ROC, Accuracy).\n",
    "    * Se generan predicciones de probabilidad y clase para el conjunto de prueba.\n",
    "    * Se muestra un **`classification_report`** (con precisión, recall, f1-score por clase) y una **`confusion_matrix`** para un análisis detallado del rendimiento, especialmente en la detección de fugas (clase 1).\n",
    "9.  **Guardado del Modelo y Escalador:**\n",
    "    * El modelo Keras entrenado se guarda en un archivo (`.keras`).\n",
    "    * El objeto `scaler` (ajustado con los datos de entrenamiento) se guarda usando `joblib`. Ambos son necesarios para hacer predicciones sobre datos nuevos en el futuro (Notebook 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89b3bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "181d5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4c299",
   "metadata": {},
   "source": [
    "## 1.1. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9ae014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Prepared Data ---\n",
      "Data loaded successfully:\n",
      "X shape: (121114, 72, 9)\n",
      "y shape: (121114,)\n"
     ]
    }
   ],
   "source": [
    "# Load Prepared Data\n",
    "print(\"--- Loading Prepared Data ---\")\n",
    "x_filename = '../data/X_sequences_unscaled.npy' \n",
    "y_filename = '../data/y_targets.npy' \n",
    "\n",
    "try:\n",
    "    X = np.load(x_filename, allow_pickle=True)\n",
    "    y = np.load(y_filename, allow_pickle=True)\n",
    "\n",
    "    print(f\"Data loaded successfully:\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure the data files are in the correct directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc88cb",
   "metadata": {},
   "source": [
    "## 1.2. División de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3abe458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting Data (Train/Validation/Test) ---\n",
      "X_train shape: (72668, 72, 9), y_train shape: (72668,)\n",
      "X_val shape: (24223, 72, 9), y_val shape: (24223,)\n",
      "X_test shape: (24223, 72, 9), y_test shape: (24223,)\n",
      "Leak % in Train: 0.8183\n",
      "Leak % in Val:   0.8184\n",
      "Leak % in Test:  0.8183\n"
     ]
    }
   ],
   "source": [
    "# Train-Validation-Test Split \n",
    "print(\"\\n--- Splitting Data (Train/Validation/Test) ---\")\n",
    "\n",
    "# Stratify ensures that the proportion of leaks (y=1) is similar across splits\n",
    "# First split: Train (60%) + Temp (40%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "# Second split: Validation (20%) + Test (20%) from Temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Verify stratification (optional)\n",
    "print(f\"Leak % in Train: {np.mean(y_train):.4f}\")\n",
    "print(f\"Leak % in Val:   {np.mean(y_val):.4f}\")\n",
    "print(f\"Leak % in Test:  {np.mean(y_test):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5eae44",
   "metadata": {},
   "source": [
    "## 1.3. Escalado de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2740e01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scaling Features (MinMaxScaler) ---\n",
      "Fitting scaler on training data...\n",
      "Transforming train, validation, and test data...\n",
      "Scaling complete. Data reshaped back to 3D.\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling (Fit on Train, Transform All) \n",
    "print(\"\\n--- Scaling Features (MinMaxScaler) ---\")\n",
    "scaler = MinMaxScaler()\n",
    "num_features = X_train.shape[2] # Number of features per time step\n",
    "\n",
    "# Reshape data for scaler: (samples * timesteps, features)\n",
    "# This scales each feature across all time steps consistently\n",
    "X_train_reshaped = X_train.reshape(-1, num_features)\n",
    "X_val_reshaped = X_val.reshape(-1, num_features)\n",
    "X_test_reshaped = X_test.reshape(-1, num_features)\n",
    "\n",
    "# Fit scaler ONLY on the training data\n",
    "print(\"Fitting scaler on training data...\")\n",
    "scaler.fit(X_train_reshaped)\n",
    "\n",
    "# Transform all datasets\n",
    "print(\"Transforming train, validation, and test data...\")\n",
    "X_train_scaled_reshaped = scaler.transform(X_train_reshaped)\n",
    "X_val_scaled_reshaped = scaler.transform(X_val_reshaped)\n",
    "X_test_scaled_reshaped = scaler.transform(X_test_reshaped)\n",
    "\n",
    "# Reshape data back to 3D format: (samples, timesteps, features)\n",
    "X_train_scaled = X_train_scaled_reshaped.reshape(X_train.shape)\n",
    "X_val_scaled = X_val_scaled_reshaped.reshape(X_val.shape)\n",
    "X_test_scaled = X_test_scaled_reshaped.reshape(X_test.shape)\n",
    "print(\"Scaling complete. Data reshaped back to 3D.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6178ac",
   "metadata": {},
   "source": [
    "## 1.4. Definición de la arquitectura del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e6906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Defining Model Architecture (Hybrid LSTM + MLP) ---\n",
      "Model architecture defined:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_lstm_sequence             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,944</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_fuga (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_lstm_sequence             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m9\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m18,944\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_dropout (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_layer_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_fuga (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,057</span> (82.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,057\u001b[0m (82.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,057</span> (82.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,057\u001b[0m (82.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Model Architecture (Hybrid LSTM + MLP) \n",
    "# Adapt this architecture based on your final feature set\n",
    "print(\"--- Defining Model Architecture (Hybrid LSTM + MLP) ---\")\n",
    "\n",
    "N_TIMESTEPS = X_train_scaled.shape[1] \n",
    "N_FEATURES = X_train_scaled.shape[2] \n",
    "\n",
    "# Define Input Layers\n",
    "# Input A: Sequence for LSTM (All features in this case)\n",
    "input_sequence = Input(shape=(N_TIMESTEPS, N_FEATURES), name='input_lstm_sequence')\n",
    "\n",
    "# --- LSTM Path ---\n",
    "lstm_out = LSTM(64, return_sequences=False, name='lstm_layer')(input_sequence) \n",
    "lstm_out = Dropout(0.3, name='lstm_dropout')(lstm_out)\n",
    "\n",
    "# In this simplified hybrid, the LSTM output directly feeds the final layers\n",
    "combined_out = Dense(32, activation='relu', name='dense_layer_1')(lstm_out)\n",
    "output = Dense(1, activation='sigmoid', name='output_fuga')(combined_out) \n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=input_sequence, outputs=output)\n",
    "\n",
    "print(\"Model architecture defined:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b0273a",
   "metadata": {},
   "source": [
    "## 1.5. Manejo del desbalanceo de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cca059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Class Weights ---\n",
      "Class weights computed: {0: np.float64(2.752367244905689), 1: np.float64(0.610994332991407)}\n"
     ]
    }
   ],
   "source": [
    "# Handle Class Imbalance \n",
    "print(\"--- Calculating Class Weights ---\")\n",
    "\n",
    "# Calculate weights to give more importance to the minority class (leaks)\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"Class weights computed: {class_weights_dict}\")\n",
    "# Example: {0: 0.501, 1: 25.0} (gives ~50x more weight to leaks if they are ~2% of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221c9c4",
   "metadata": {},
   "source": [
    "## 1.6. Compilación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa568d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Compiling Model ---\n",
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "# Compile Model\n",
    "print(\"--- Compiling Model ---\")\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(curve='PR', name='auc_pr'), \n",
    "                       tf.keras.metrics.AUC(name='auc_roc'),\n",
    "                       'accuracy'])\n",
    "print(\"Model compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908b7f1",
   "metadata": {},
   "source": [
    "## 1.7. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ddfad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training ---\n",
      "Epoch 1/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 25ms/step - accuracy: 0.7418 - auc_pr: 0.9203 - auc_roc: 0.7489 - loss: 0.5855 - val_accuracy: 0.8508 - val_auc_pr: 0.9499 - val_auc_roc: 0.8278 - val_loss: 0.5173\n",
      "Epoch 2/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 29ms/step - accuracy: 0.8529 - auc_pr: 0.9430 - auc_roc: 0.8175 - loss: 0.5024 - val_accuracy: 0.8537 - val_auc_pr: 0.9505 - val_auc_roc: 0.8294 - val_loss: 0.4906\n",
      "Epoch 3/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 26ms/step - accuracy: 0.8516 - auc_pr: 0.9451 - auc_roc: 0.8206 - loss: 0.4986 - val_accuracy: 0.8567 - val_auc_pr: 0.9527 - val_auc_roc: 0.8386 - val_loss: 0.5210\n",
      "Epoch 4/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 27ms/step - accuracy: 0.8546 - auc_pr: 0.9471 - auc_roc: 0.8250 - loss: 0.4940 - val_accuracy: 0.8596 - val_auc_pr: 0.9544 - val_auc_roc: 0.8441 - val_loss: 0.4847\n",
      "Epoch 5/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 27ms/step - accuracy: 0.8572 - auc_pr: 0.9500 - auc_roc: 0.8342 - loss: 0.4843 - val_accuracy: 0.8577 - val_auc_pr: 0.9539 - val_auc_roc: 0.8434 - val_loss: 0.4635\n",
      "Epoch 6/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 27ms/step - accuracy: 0.8586 - auc_pr: 0.9518 - auc_roc: 0.8378 - loss: 0.4794 - val_accuracy: 0.8643 - val_auc_pr: 0.9565 - val_auc_roc: 0.8507 - val_loss: 0.4584\n",
      "Epoch 7/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 28ms/step - accuracy: 0.8304 - auc_pr: 0.9435 - auc_roc: 0.8138 - loss: 0.5112 - val_accuracy: 0.8531 - val_auc_pr: 0.9515 - val_auc_roc: 0.8330 - val_loss: 0.5180\n",
      "Epoch 8/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 29ms/step - accuracy: 0.8566 - auc_pr: 0.9496 - auc_roc: 0.8317 - loss: 0.4860 - val_accuracy: 0.8597 - val_auc_pr: 0.9572 - val_auc_roc: 0.8540 - val_loss: 0.4685\n",
      "Epoch 9/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 29ms/step - accuracy: 0.8605 - auc_pr: 0.9518 - auc_roc: 0.8360 - loss: 0.4790 - val_accuracy: 0.8589 - val_auc_pr: 0.9535 - val_auc_roc: 0.8388 - val_loss: 0.4578\n",
      "Epoch 10/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 29ms/step - accuracy: 0.8617 - auc_pr: 0.9539 - auc_roc: 0.8410 - loss: 0.4716 - val_accuracy: 0.8622 - val_auc_pr: 0.9592 - val_auc_roc: 0.8580 - val_loss: 0.4619\n",
      "Epoch 11/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 29ms/step - accuracy: 0.8635 - auc_pr: 0.9574 - auc_roc: 0.8519 - loss: 0.4591 - val_accuracy: 0.8624 - val_auc_pr: 0.9606 - val_auc_roc: 0.8600 - val_loss: 0.4346\n",
      "Epoch 12/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 28ms/step - accuracy: 0.8619 - auc_pr: 0.9550 - auc_roc: 0.8446 - loss: 0.4640 - val_accuracy: 0.8636 - val_auc_pr: 0.9584 - val_auc_roc: 0.8544 - val_loss: 0.4656\n",
      "Epoch 13/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 28ms/step - accuracy: 0.8629 - auc_pr: 0.9531 - auc_roc: 0.8408 - loss: 0.4627 - val_accuracy: 0.8645 - val_auc_pr: 0.9584 - val_auc_roc: 0.8552 - val_loss: 0.4394\n",
      "Epoch 14/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 27ms/step - accuracy: 0.8634 - auc_pr: 0.9552 - auc_roc: 0.8471 - loss: 0.4620 - val_accuracy: 0.8639 - val_auc_pr: 0.9609 - val_auc_roc: 0.8626 - val_loss: 0.4357\n",
      "Epoch 15/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 28ms/step - accuracy: 0.8616 - auc_pr: 0.9579 - auc_roc: 0.8536 - loss: 0.4545 - val_accuracy: 0.8686 - val_auc_pr: 0.9610 - val_auc_roc: 0.8670 - val_loss: 0.4377\n",
      "Epoch 16/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 28ms/step - accuracy: 0.8648 - auc_pr: 0.9584 - auc_roc: 0.8552 - loss: 0.4505 - val_accuracy: 0.8680 - val_auc_pr: 0.9580 - val_auc_roc: 0.8519 - val_loss: 0.4235\n",
      "Epoch 17/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.8681 - auc_pr: 0.9594 - auc_roc: 0.8578 - loss: 0.4458 - val_accuracy: 0.8680 - val_auc_pr: 0.9616 - val_auc_roc: 0.8639 - val_loss: 0.4344\n",
      "Epoch 18/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 53ms/step - accuracy: 0.8654 - auc_pr: 0.9571 - auc_roc: 0.8507 - loss: 0.4563 - val_accuracy: 0.8535 - val_auc_pr: 0.9357 - val_auc_roc: 0.7804 - val_loss: 0.5313\n",
      "Epoch 19/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 44ms/step - accuracy: 0.8404 - auc_pr: 0.9513 - auc_roc: 0.8318 - loss: 0.4845 - val_accuracy: 0.8693 - val_auc_pr: 0.9614 - val_auc_roc: 0.8622 - val_loss: 0.4368\n",
      "Epoch 20/20\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.8676 - auc_pr: 0.9592 - auc_roc: 0.8569 - loss: 0.4453 - val_accuracy: 0.8700 - val_auc_pr: 0.9623 - val_auc_roc: 0.8658 - val_loss: 0.4028\n",
      "Model training finished.\n"
     ]
    }
   ],
   "source": [
    "# Train Model with Early Stopping\n",
    "print(\"--- Starting Model Training ---\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20 # Start with fewer epochs, increase if needed\n",
    "\n",
    "# Add EarlyStopping to prevent overfitting and save the best model\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc_pr', \n",
    "    patience=5,          \n",
    "    mode='max',            \n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    class_weight=class_weights_dict, \n",
    "    callbacks=[early_stopping]       \n",
    ")\n",
    "\n",
    "# Training complete\n",
    "print(\"Model training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67686f",
   "metadata": {},
   "source": [
    "## 1.8. Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff2f4a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Model on Test Data ---\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.8686 - auc_pr: 0.9620 - auc_roc: 0.8645 - loss: 0.4038\n",
      "Test Loss: 0.4038\n",
      "Test AUC-PR: 0.9620\n",
      "Test AUC-ROC: 0.8645\n",
      "Test Accuracy: 0.8686\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Fuga (0)       0.63      0.67      0.65      4401\n",
      "    Fuga (1)       0.93      0.91      0.92     19822\n",
      "\n",
      "    accuracy                           0.87     24223\n",
      "   macro avg       0.78      0.79      0.78     24223\n",
      "weighted avg       0.87      0.87      0.87     24223\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[ 2962  1439]\n",
      " [ 1744 18078]]\n",
      "\n",
      "Average Precision Score (Test Set): 0.9621\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model on Test Data \n",
    "print(\"--- Evaluating Model on Test Data ---\")\n",
    "results = model.evaluate(X_test_scaled, y_test, batch_size=BATCH_SIZE)\n",
    "print(f\"Test Loss: {results[0]:.4f}\")\n",
    "print(f\"Test AUC-PR: {results[1]:.4f}\")\n",
    "print(f\"Test AUC-ROC: {results[2]:.4f}\")\n",
    "print(f\"Test Accuracy: {results[3]:.4f}\")\n",
    "\n",
    "# Get predictions for detailed metrics\n",
    "y_pred_proba = model.predict(X_test_scaled).ravel() \n",
    "y_pred_class = (y_pred_proba > 0.5).astype(int)     \n",
    "\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "try:\n",
    "    print(classification_report(\n",
    "        y_test,\n",
    "        y_pred_class,\n",
    "        target_names=['No Fuga (0)', 'Fuga (1)'],\n",
    "        labels=[0, 1], \n",
    "    ))\n",
    "\n",
    "except ValueError as e:\n",
    "     print(f\"Error generating classification report even with labels: {e}\")\n",
    "     print(\"Checking unique values:\")\n",
    "     print(\"Unique values in y_test:\", np.unique(y_test))\n",
    "     print(\"Unique values in y_pred_class:\", np.unique(y_pred_class))\n",
    "\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "# Rows: Actual, Columns: Predicted\n",
    "# [[TN, FP],\n",
    "#  [FN, TP]]\n",
    "print(confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "# Calculate Average Precision Score (equivalent to AUC-PR) again for confirmation\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAverage Precision Score (Test Set): {avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4482802",
   "metadata": {},
   "source": [
    "## 1.9. Guardado del modelo y escalador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d187fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Model and Scaler ---\n",
      "Model saved successfully to: ../data/gesai_lstm_model.keras\n",
      "Scaler saved successfully to: ../data/gesai_scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save the Trained Model and Scaler \n",
    "print(\"\\n--- Saving Model and Scaler ---\")\n",
    "model_filename = '../data/gesai_lstm_model.keras' \n",
    "scaler_filename = '../data/gesai_scaler.joblib'\n",
    "\n",
    "try:\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved successfully to: {model_filename}\")\n",
    "\n",
    "    # Save the scaler\n",
    "    import joblib\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"Scaler saved successfully to: {scaler_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving model or scaler: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
