{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5326d43e",
   "metadata": {},
   "source": [
    "# Data Preparation for GeSAI - AB Data Challenge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d86dd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4245eb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando 4 datasets geográficos...\n",
      "Uniendo archivos geográficos...\n",
      "Tabla de características GEO creada con 1069 filas.\n",
      "Cargando datos de AEMET...\n",
      "Generando datos de Festivos...\n",
      "Cargando archivo principal (76M filas) con DASK...\n",
      "Creando 60 trozos más pequeños...\n",
      "Creando columnas de Fuga...\n",
      "Separando Fecha y Hora...\n",
      "Preparando Merge 1 (Meteo + Festivos)...\n",
      "Preparando Merge 2 (Geográfico)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barco\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\multi.py:169: UserWarning: Merging dataframes with merge column data type mismatches: \n",
      "+----------------------------------+------------+-------------+\n",
      "| Merge columns                    | left dtype | right dtype |\n",
      "+----------------------------------+------------+-------------+\n",
      "| ('KEY_DISTRITO', 'KEY_DISTRITO') | string     | string      |\n",
      "| ('KEY_SECCION', 'KEY_SECCION')   | string     | string      |\n",
      "+----------------------------------+------------+-------------+\n",
      "Cast dtypes explicitly to avoid unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpiando nulos finales...\n",
      "¡EJECUTANDO! Dask está procesando TODO y guardando en disco...\n",
      "¡HECHO! Tu dataset final y completo está en la CARPETA: 'dataset_FINAL_COMPLETO/'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import gc\n",
    "\n",
    "# --- 0. Configuración General ---\n",
    "MAIN_PARQUET = '../data/official-data/data_ab3_complete.parquet'\n",
    "OUTPUT_PARQUET_DIR = '../data/processed-data/dataset_FINAL_COMPLETO/'\n",
    "NUMERO_DE_TROZOS = 60\n",
    "LLAVE_DISTRITO = 'KEY_DISTRITO'\n",
    "LLAVE_SECCION = 'KEY_SECCION'\n",
    "\n",
    "\n",
    "RENTA_CSV = '../data/open-data/renda_procesada.csv'\n",
    "ANTIGUITAD_CSV = '../data/open-data/antiguitat_pivotada.csv'\n",
    "POBLACION_CSV = '../data/open-data/poblacion_pivotada.csv'\n",
    "OBRES_CSV = '../data/open-data/obres_procesadas.csv'\n",
    "# --- FIN DE LA ACTUALIZACIÓN ---\n",
    "\n",
    "# --- 1. Cargar y Preparar Tablas Pequeñas (con PANDAS) ---\n",
    "print(\"Cargando 4 datasets geográficos...\")\n",
    "dtype_llaves = {LLAVE_DISTRITO: 'string', LLAVE_SECCION: 'string'}\n",
    "df_renta = pd.read_csv(RENTA_CSV, dtype=dtype_llaves)\n",
    "df_antiguitat = pd.read_csv(ANTIGUITAD_CSV, dtype=dtype_llaves)\n",
    "df_poblacion = pd.read_csv(POBLACION_CSV, dtype=dtype_llaves)\n",
    "df_obres = pd.read_csv(OBRES_CSV, dtype=dtype_llaves)\n",
    "\n",
    "print(\"Uniendo archivos geográficos...\")\n",
    "df_geo_total = df_renta.merge(df_antiguitat, on=[LLAVE_DISTRITO, LLAVE_SECCION], how='outer')\n",
    "df_geo_total = df_geo_total.merge(df_poblacion, on=[LLAVE_DISTRITO, LLAVE_SECCION], how='outer')\n",
    "df_geo_total = df_geo_total.merge(df_obres, on=[LLAVE_DISTRITO, LLAVE_SECCION], how='outer')\n",
    "nuevas_columnas_geo = df_geo_total.columns.drop([LLAVE_DISTRITO, LLAVE_SECCION]).tolist()\n",
    "df_geo_total = df_geo_total.fillna(0)\n",
    "print(f\"Tabla de características GEO creada con {len(df_geo_total)} filas.\")\n",
    "\n",
    "# 1B: Preparar datos METEO (Del Notebook)\n",
    "print(\"Cargando datos de AEMET...\")\n",
    "df_aemet1 = pd.read_json('../data/open-data/data_aemet_1.json')\n",
    "df_aemet2 = pd.read_json('../data/open-data/data_aemet_2.json')\n",
    "df_aemet = pd.concat([df_aemet1, df_aemet2], ignore_index=True)\n",
    "df_aemet = df_aemet[['fecha', 'tmed', 'tmin', 'tmax', 'prec', 'hrMedia']]\n",
    "df_aemet.fillna({'prec': 0}, inplace=True)\n",
    "cols_to_rename = {\n",
    "    'fecha': 'FECHA', 'tmed': 'TEMP_MEDIA', 'tmin': 'TEMP_MIN',\n",
    "    'tmax': 'TEMP_MAX', 'prec': 'PRECIPITACION', 'hrMedia': 'HUMEDAD_RELATIVA_MEDIA'\n",
    "}\n",
    "df_aemet.rename(columns=cols_to_rename, inplace=True)\n",
    "df_aemet['FECHA'] = pd.to_datetime(df_aemet['FECHA'], errors='coerce')\n",
    "nuevas_columnas_meteo = list(cols_to_rename.values())[1:]\n",
    "\n",
    "# 1C: Preparar datos FESTIVOS (Del Notebook)\n",
    "print(\"Generando datos de Festivos...\")\n",
    "fechas = pd.date_range(start='2024-01-01', end='2024-12-31')\n",
    "df_fechas = pd.DataFrame({'FECHA': fechas})\n",
    "es_holidays = holidays.CountryHoliday('ES', subdiv='CT', years=2024)\n",
    "df_fechas['FESTIVO'] = df_fechas['FECHA'].apply(lambda date: date in es_holidays)\n",
    "def get_tipo_dia_simple(fecha, es_festivo):\n",
    "    if es_festivo: return 'Festivo'\n",
    "    elif fecha.weekday() >= 5: return 'Fin de Semana'\n",
    "    else: return 'Laborable'\n",
    "df_fechas['TIPO_DIA'] = df_fechas.apply(lambda row: get_tipo_dia_simple(row['FECHA'], row['FESTIVO']), axis=1)\n",
    "df_fechas['FECHA'] = pd.to_datetime(df_fechas['FECHA'], errors='coerce')\n",
    "nuevas_columnas_festivos = ['FESTIVO', 'TIPO_DIA']\n",
    "\n",
    "# 1D: Unir tablas pequeñas de Meteo y Festivos\n",
    "df_meteo_festivos = pd.merge(df_aemet, df_fechas, on='FECHA', how='outer')\n",
    "\n",
    "# --- 2. Cargar y Preparar Tabla Gigante (con DASK) ---\n",
    "print(\"Cargando archivo principal (76M filas) con DASK...\")\n",
    "df_main_dask = dd.read_parquet(MAIN_PARQUET)\n",
    "print(f\"Creando {NUMERO_DE_TROZOS} trozos más pequeños...\")\n",
    "df_main_dask = df_main_dask.repartition(npartitions=NUMERO_DE_TROZOS)\n",
    "\n",
    "# 2A: Limpiar SECCIO_CENSAL\n",
    "df_main_dask = df_main_dask.dropna(subset=['SECCIO_CENSAL'])\n",
    "\n",
    "# 2B: Imputar CONSUMO_REAL\n",
    "df_main_dask['CONSUMO_REAL'] = df_main_dask['CONSUMO_REAL'].fillna(0)\n",
    "\n",
    "# 2C: Crear Columnas FUGA (Manejando nulos)\n",
    "def crear_columnas_fuga(df_particion):\n",
    "    df_particion['FUGA_DETECTADA'] = df_particion['CODIGO_MENSAJE'].apply(\n",
    "        lambda x: 0 if pd.isna(x) else (1 if x in ['FUITA', 'REITERACIÓ DE FUITA'] else 0)\n",
    "    )\n",
    "    df_particion['FUGA_REITERADA'] = df_particion['CODIGO_MENSAJE'].apply(\n",
    "        lambda x: 0 if pd.isna(x) else (1 if x == 'REITERACIÓ DE FUITA' else 0)\n",
    "    )\n",
    "    return df_particion\n",
    "\n",
    "print(\"Creando columnas de Fuga...\")\n",
    "new_meta = df_main_dask._meta.copy()\n",
    "new_meta['FUGA_DETECTADA'] = pd.Series(dtype='int64')\n",
    "new_meta['FUGA_REITERADA'] = pd.Series(dtype='int64')\n",
    "df_main_dask = df_main_dask.map_partitions(crear_columnas_fuga, meta=new_meta)\n",
    "df_main_dask = df_main_dask.drop(columns=['CREATED_MENSAJE', 'CODIGO_MENSAJE', 'TIPO_MENSAJE'])\n",
    "\n",
    "# 2D: Separar FECHA_HORA\n",
    "print(\"Separando Fecha y Hora...\")\n",
    "df_main_dask['FECHA'] = df_main_dask['FECHA_HORA'].dt.date\n",
    "df_main_dask['HORA'] = df_main_dask['FECHA_HORA'].dt.time.astype(str) # Corrección V12\n",
    "df_main_dask['FECHA'] = dd.to_datetime(df_main_dask['FECHA'], errors='coerce')\n",
    "\n",
    "# --- 3. MERGES FINALES (con DASK) ---\n",
    "print(\"Preparando Merge 1 (Meteo + Festivos)...\")\n",
    "df_main_dask = dd.merge(\n",
    "    df_main_dask,\n",
    "    df_meteo_festivos,\n",
    "    on='FECHA',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Preparando Merge 2 (Geográfico)...\")\n",
    "llave_str = df_main_dask['SECCIO_CENSAL'].astype('string')\n",
    "df_main_dask[LLAVE_DISTRITO] = llave_str.str.slice(5, 7).astype('string')\n",
    "df_main_dask[LLAVE_SECCION] = llave_str.str.slice(7, 10).astype('string')\n",
    "\n",
    "df_final_dask = dd.merge(\n",
    "    df_main_dask,\n",
    "    df_geo_total,\n",
    "    on=[LLAVE_DISTRITO, LLAVE_SECCION],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- 4. Limpieza y Guardado Final ---\n",
    "del df_renta, df_antiguitat, df_poblacion, df_obres, df_geo_total, df_aemet, df_fechas, df_meteo_festivos\n",
    "gc.collect()\n",
    "\n",
    "print(\"Limpiando nulos finales...\")\n",
    "fill_values_geo = {col: 0 for col in nuevas_columnas_geo}\n",
    "fill_values_meteo = {col: 0 for col in nuevas_columnas_meteo}\n",
    "fill_values_festivos = {'FESTIVO': False, 'TIPO_DIA': 'Laborable'} \n",
    "\n",
    "df_final_dask = df_final_dask.fillna(value=fill_values_geo)\n",
    "df_final_dask = df_final_dask.fillna(value=fill_values_meteo)\n",
    "df_final_dask = df_final_dask.fillna(value=fill_values_festivos)\n",
    "df_final_dask['FESTIVO'] = df_final_dask['FESTIVO'].astype(bool)\n",
    "\n",
    "# --- 5. Ejecutar y Guardado ---\n",
    "print(\"¡EJECUTANDO! Dask está procesando TODO y guardando en disco...\")\n",
    "df_final_dask.to_parquet(OUTPUT_PARQUET_DIR, write_index=False)\n",
    "\n",
    "print(f\"¡HECHO! Tu dataset final y completo está en la CARPETA: '{OUTPUT_PARQUET_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf530e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando duplicados finales...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_final_dask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m subset_duplicados \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOLISSA_SUBM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFECHA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHORA\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 2. Eliminamos duplicados en Dask\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# (Dask necesita saber cómo ordenar para decidir cuál quedarse, \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#  por defecto se queda con el primero de la partición)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df_final_dask \u001b[38;5;241m=\u001b[39m df_final_dask\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39msubset_duplicados)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# --- Exportación ---\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5. INICIANDO CÓMPUTO FINAL y Guardado...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_final_dask' is not defined"
     ]
    }
   ],
   "source": [
    "# --- EN TU NOTEBOOK 1 (ETL con Dask) ---\n",
    "\n",
    "# ... (después de todos los merges y limpiezas) ...\n",
    "\n",
    "print(\"Eliminando duplicados finales...\")\n",
    "\n",
    "# 1. Definimos qué constituye un duplicado \"real\"\n",
    "# Un cliente no debería tener dos filas para la misma hora exacta.\n",
    "subset_duplicados = ['POLISSA_SUBM', 'FECHA', 'HORA']\n",
    "\n",
    "# 2. Eliminamos duplicados en Dask\n",
    "# (Dask necesita saber cómo ordenar para decidir cuál quedarse, \n",
    "#  por defecto se queda con el primero de la partición)\n",
    "df_final_dask = df_final_dask.drop_duplicates(subset=subset_duplicados)\n",
    "\n",
    "# --- Exportación ---\n",
    "print(\"5. INICIANDO CÓMPUTO FINAL y Guardado...\")\n",
    "df_final_dask.to_parquet(OUTPUT_PARQUET_DIR, write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
