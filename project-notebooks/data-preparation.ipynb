{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5326d43e",
   "metadata": {},
   "source": [
    "# 1. Data preparation GeSAI: Pipeline de integración (Pandas + Dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e642b",
   "metadata": {},
   "source": [
    "Este notebook gestiona la preparación final de los datos. Debido al gran volumen del dataset principal de consumo (aprox. 76 millones de filas), utilizamos una estrategia híbrida:\n",
    "\n",
    "* Pandas: Para procesar datasets auxiliares \"pequeños\" (Geográficos, Meteorológicos y Festivos).\n",
    "\n",
    "* Dask: Para procesar y fusionar el dataset masivo de consumo sin saturar la memoria RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea19aee",
   "metadata": {},
   "source": [
    "## 1.1. Configuración inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b0649",
   "metadata": {},
   "source": [
    "Cargamos las librerías necesarias y definimos las rutas de archivos y constantes globales. Esto facilita la modificación de rutas en el futuro sin tocar la lógica del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86dd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import gc\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27548201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las rutas y constantes\n",
    "MAIN_PARQUET = '../data/official-data/data_ab3_complete.parquet'\n",
    "OUTPUT_PARQUET_DIR = '../data/processed-data/dataset_FINAL_COMPLETO/'\n",
    "\n",
    "NUMERO_DE_TROZOS = 60\n",
    "\n",
    "LLAVE_DISTRITO = 'KEY_DISTRITO'\n",
    "LLAVE_SECCION = 'KEY_SECCION'\n",
    "\n",
    "\n",
    "RENTA_CSV = '../data/open-data/renda_procesada.csv'\n",
    "ANTIGUITAD_CSV = '../data/open-data/antiguitat_pivotada.csv'\n",
    "POBLACION_CSV = '../data/open-data/poblacion_pivotada.csv'\n",
    "OBRES_CSV = '../data/open-data/obres_procesadas.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c8870",
   "metadata": {},
   "source": [
    "## 1.2. Preparación de datos auxiliares (In-Memory con Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efaed8",
   "metadata": {},
   "source": [
    "En esta fase preparamos todas las tablas que son lo suficientemente pequeñas para caber en memoria. Estas actuarán como tablas de dimensiones (\"lookup tables\") para enriquecer el dataset principal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fea7be",
   "metadata": {},
   "source": [
    "### 1.2.1. Procesamiento de datos geográficos de renta, antigüedad, población y obras públicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670e58f5",
   "metadata": {},
   "source": [
    "Cargamos los datasets de renta, antigüedad de edificios, población y obras. Los unificamos en una sola \"Tabla Maestra Geográfica\" indexada por Distrito y Sección Censal.\n",
    "\n",
    "Objetivo: Tener un perfil socioeconómico y urbano único para cada sección censal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4245eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos y preparamos los datos geográficos \n",
    "dtype_llaves = {LLAVE_DISTRITO: 'string', LLAVE_SECCION: 'string'}\n",
    "df_renta = pd.read_csv(RENTA_CSV, dtype=dtype_llaves)\n",
    "df_antiguitat = pd.read_csv(ANTIGUITAD_CSV, dtype=dtype_llaves)\n",
    "df_poblacion = pd.read_csv(POBLACION_CSV, dtype=dtype_llaves)\n",
    "df_obres = pd.read_csv(OBRES_CSV, dtype=dtype_llaves)\n",
    "\n",
    "# Fusionamos los datos geográficos en un solo DataFrame\n",
    "df_geo_total = df_renta.merge(df_antiguitat, on=[LLAVE_DISTRITO, LLAVE_SECCION], how='outer')\n",
    "df_geo_total = df_geo_total.merge(df_poblacion, on=[LLAVE_DISTRITO, LLAVE_SECCION], how='outer')\n",
    "df_geo_total = df_geo_total.merge(df_obres, on=[LLAVE_DISTRITO, LLAVE_SECCION], how='outer')\n",
    "nuevas_columnas_geo = df_geo_total.columns.drop([LLAVE_DISTRITO, LLAVE_SECCION]).tolist()\n",
    "df_geo_total = df_geo_total.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c48a6",
   "metadata": {},
   "source": [
    "### 1.2.2. Procesamiento de datos meteorológicos (AEMET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e3f25",
   "metadata": {},
   "source": [
    "Cargamos los JSONs de la AEMET, concatenamos los históricos y limpiamos los nombres de columnas.\n",
    "\n",
    "Objetivo: Asociar condiciones climáticas (temperatura, lluvia) a cada fecha, ya que influyen en el consumo y en las roturas por cambios térmicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a14128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de datos meteorológicos (AEMET)\n",
    "df_aemet1 = pd.read_json('../data/open-data/data_aemet_1.json')\n",
    "df_aemet2 = pd.read_json('../data/open-data/data_aemet_2.json')\n",
    "\n",
    "# Concatenamos ambos dataframes\n",
    "df_aemet = pd.concat([df_aemet1, df_aemet2], ignore_index=True)\n",
    "\n",
    "# Seleccionamos columnas relevantes\n",
    "df_aemet = df_aemet[['fecha', 'tmed', 'tmin', 'tmax', 'prec', 'hrMedia']]\n",
    "\n",
    "# Rellenamos nulos de precipitación con 0\n",
    "df_aemet.fillna({'prec': 0}, inplace=True)\n",
    "\n",
    "# Renombramos columnas y convertimos tipos\n",
    "cols_to_rename = {\n",
    "    'fecha': 'FECHA', 'tmed': 'TEMP_MEDIA', 'tmin': 'TEMP_MIN',\n",
    "    'tmax': 'TEMP_MAX', 'prec': 'PRECIPITACION', 'hrMedia': 'HUMEDAD_RELATIVA_MEDIA'\n",
    "}\n",
    "\n",
    "# Renombramos columnas y convertimos tipos\n",
    "df_aemet.rename(columns=cols_to_rename, inplace=True)\n",
    "df_aemet['FECHA'] = pd.to_datetime(df_aemet['FECHA'], errors='coerce')\n",
    "\n",
    "# Lista de nuevas columnas meteorológicas\n",
    "nuevas_columnas_meteo = list(cols_to_rename.values())[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584adb70",
   "metadata": {},
   "source": [
    "### 1.2.3. Generación de calendario de festivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfd615",
   "metadata": {},
   "source": [
    "Utilizamos la librería holidays para generar un calendario específico de Cataluña para el año 2024.\n",
    "\n",
    "Objetivo: Diferenciar días laborables de festivos/fines de semana, ya que el patrón de consumo humano varía drásticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db62faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de calendario de festivos\n",
    "fechas = pd.date_range(start='2024-01-01', end='2024-12-31')\n",
    "df_fechas = pd.DataFrame({'FECHA': fechas})\n",
    "\n",
    "# Cálculo de festivos en Cataluña para 2024\n",
    "es_holidays = holidays.CountryHoliday('ES', subdiv='CT', years=2024)\n",
    "df_fechas['FESTIVO'] = df_fechas['FECHA'].apply(lambda date: date in es_holidays)\n",
    "\n",
    "# Clasificación del tipo de día\n",
    "def get_tipo_dia_simple(fecha, es_festivo):\n",
    "    if es_festivo: return 'Festivo'\n",
    "    elif fecha.weekday() >= 5: return 'Fin de Semana'\n",
    "    else: return 'Laborable'\n",
    "\n",
    "# Aplicamos la función para obtener el tipo de día    \n",
    "df_fechas['TIPO_DIA'] = df_fechas.apply(lambda row: get_tipo_dia_simple(row['FECHA'], row['FESTIVO']), axis=1)\n",
    "df_fechas['FECHA'] = pd.to_datetime(df_fechas['FECHA'], errors='coerce')\n",
    "\n",
    "# Lista de nuevas columnas de festivos\n",
    "nuevas_columnas_festivos = ['FESTIVO', 'TIPO_DIA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c900fa",
   "metadata": {},
   "source": [
    "### 1.2.4. Unificación de datos temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4329b",
   "metadata": {},
   "source": [
    "Fusionamos la meteorología y el calendario en una sola tabla auxiliar indexada por fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7521aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Meteo + Festivos\n",
    "df_meteo_festivos = pd.merge(df_aemet, df_fechas, on='FECHA', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c089a",
   "metadata": {},
   "source": [
    "## 1.3. Procesamiento masivo del dataset principal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54fb226",
   "metadata": {},
   "source": [
    "A partir de aquí utilizamos Dask. Cargamos el dataset principal (Parquet), lo dividimos en particiones para paralelizar el trabajo y aplicamos transformaciones de limpieza fila por fila."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe37e28",
   "metadata": {},
   "source": [
    "### 1.3.1. Carga y limpieza inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727953f",
   "metadata": {},
   "source": [
    "Cargamos el archivo gigante y eliminamos registros inválidos (sin sección censal) e imputamos consumos nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54d550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga y limpieza inicial\n",
    "df_main_dask = dd.read_parquet(MAIN_PARQUET)\n",
    "df_main_dask = df_main_dask.repartition(npartitions=NUMERO_DE_TROZOS)\n",
    "\n",
    "# Limpieza de datos faltantes\n",
    "df_main_dask = df_main_dask.dropna(subset=['SECCIO_CENSAL'])\n",
    "\n",
    "# Relleno de valores faltantes en CONSUMO_REAL con 0\n",
    "df_main_dask['CONSUMO_REAL'] = df_main_dask['CONSUMO_REAL'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b339faa",
   "metadata": {},
   "source": [
    "### 1.3.2. Ingeniería de características (Target y fechas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99a833",
   "metadata": {},
   "source": [
    "Definimos una función para transformar las columnas de \"mensajes\" en columnas binarias útiles para Machine Learning (Target: Fuga Sí/No) y separamos la fecha de la hora.\n",
    "\n",
    "Nota: Usamos map_partitions para aplicar esta función de manera eficiente a cada trozo del dataframe distribuido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d46a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de columnas FUGA_DETECTADA y FUGA_REITERADA\n",
    "def crear_columnas_fuga(df_particion):\n",
    "    df_particion['FUGA_DETECTADA'] = df_particion['CODIGO_MENSAJE'].apply(\n",
    "        lambda x: 0 if pd.isna(x) else (1 if x in ['FUITA', 'REITERACIÓ DE FUITA'] else 0)\n",
    "    )\n",
    "    df_particion['FUGA_REITERADA'] = df_particion['CODIGO_MENSAJE'].apply(\n",
    "        lambda x: 0 if pd.isna(x) else (1 if x == 'REITERACIÓ DE FUITA' else 0)\n",
    "    )\n",
    "    return df_particion\n",
    "\n",
    "# Aplicamos la función a cada partición del DataFrame Dask\n",
    "new_meta = df_main_dask._meta.copy()\n",
    "new_meta['FUGA_DETECTADA'] = pd.Series(dtype='int64')\n",
    "new_meta['FUGA_REITERADA'] = pd.Series(dtype='int64')\n",
    "df_main_dask = df_main_dask.map_partitions(crear_columnas_fuga, meta=new_meta)\n",
    "df_main_dask = df_main_dask.drop(columns=['CREATED_MENSAJE', 'CODIGO_MENSAJE', 'TIPO_MENSAJE'])\n",
    "\n",
    "# Extracción de fecha y hora\n",
    "df_main_dask['FECHA'] = df_main_dask['FECHA_HORA'].dt.date\n",
    "df_main_dask['HORA'] = df_main_dask['FECHA_HORA'].dt.time.astype(str) # Corrección V12\n",
    "df_main_dask['FECHA'] = dd.to_datetime(df_main_dask['FECHA'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ce694",
   "metadata": {},
   "source": [
    "## 1.4. Fusión de datos (Merges distribuídos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5c551",
   "metadata": {},
   "source": [
    "Enriquecemos el dataset principal cruzándolo con las tablas auxiliares que preparamos en Pandas. Dask maneja eficientemente el cruce de una \"Tabla Gigante\" (Dask) con una \"Tabla Pequeña\" (Pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16269dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barco\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\multi.py:169: UserWarning: Merging dataframes with merge column data type mismatches: \n",
      "+----------------------------------+------------+-------------+\n",
      "| Merge columns                    | left dtype | right dtype |\n",
      "+----------------------------------+------------+-------------+\n",
      "| ('KEY_DISTRITO', 'KEY_DISTRITO') | string     | string      |\n",
      "| ('KEY_SECCION', 'KEY_SECCION')   | string     | string      |\n",
      "+----------------------------------+------------+-------------+\n",
      "Cast dtypes explicitly to avoid unexpected results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preparamos Merge 1 (Meteo + Festivos)\n",
    "df_main_dask = dd.merge(\n",
    "    df_main_dask,\n",
    "    df_meteo_festivos,\n",
    "    on='FECHA',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Preparamos Merge 2 (Datos Geográficos)\n",
    "llave_str = df_main_dask['SECCIO_CENSAL'].astype('string')\n",
    "df_main_dask[LLAVE_DISTRITO] = llave_str.str.slice(5, 7).astype('string')\n",
    "df_main_dask[LLAVE_SECCION] = llave_str.str.slice(7, 10).astype('string')\n",
    "\n",
    "# Merge Datos Geográficos\n",
    "df_final_dask = dd.merge(\n",
    "    df_main_dask,\n",
    "    df_geo_total,\n",
    "    on=[LLAVE_DISTRITO, LLAVE_SECCION],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7f21b",
   "metadata": {},
   "source": [
    "## 1.5. Limpieza final y guardado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16c445",
   "metadata": {},
   "source": [
    "Liberamos memoria RAM eliminando los DataFrames de Pandas que ya no necesitamos. \n",
    "\n",
    "Configuramos los valores por defecto para rellenar los nulos que hayan podido surgir tras los merges (ej. una fecha sin datos meteo o una sección censal sin datos de renta) y dropeamos duplicados para asegurar la consistencia del dataset. \n",
    "\n",
    "Finalmente, ejecutamos el cómputo y guardamos en disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberamos memoria RAM eliminando los DataFrames de Pandas que ya no necesitamos\n",
    "del df_renta, df_antiguitat, df_poblacion, df_obres, df_geo_total, df_aemet, df_fechas, df_meteo_festivos\n",
    "gc.collect()\n",
    "\n",
    "# Diccionarios de valores por defecto para rellenar nulos\n",
    "fill_values_geo = {col: 0 for col in nuevas_columnas_geo}\n",
    "fill_values_meteo = {col: 0 for col in nuevas_columnas_meteo}\n",
    "fill_values_festivos = {'FESTIVO': False, 'TIPO_DIA': 'Laborable'} \n",
    "\n",
    "# Limpieza final de nulos\n",
    "df_final_dask = df_final_dask.fillna(value=fill_values_geo)\n",
    "df_final_dask = df_final_dask.fillna(value=fill_values_meteo)\n",
    "df_final_dask = df_final_dask.fillna(value=fill_values_festivos)\n",
    "df_final_dask['FESTIVO'] = df_final_dask['FESTIVO'].astype(bool)\n",
    "\n",
    "# Eliminamos duplicados\n",
    "subset_duplicados = ['POLISSA_SUBM', 'FECHA', 'HORA']\n",
    "df_final_dask = df_final_dask.drop_duplicates(subset=subset_duplicados)\n",
    "\n",
    "# Aseguramos orden por fechas y horas (lo aseguraremos más adelante en el pipeline)\n",
    "#df_final_dask = df_final_dask.set_index('FECHA_HORA').reset_index()\n",
    "\n",
    "# Ejecutamos el cómputo y guardamos en disco\n",
    "df_final_dask.to_parquet(OUTPUT_PARQUET_DIR, write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
