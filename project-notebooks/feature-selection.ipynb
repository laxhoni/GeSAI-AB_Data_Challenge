{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf25e453",
   "metadata": {},
   "source": [
    "# 1. Ingeniería de Características y Creación de Secuencias LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234d171",
   "metadata": {},
   "source": [
    "Este notebook toma el dataset preprocesado (`df_final` guardado desde el Notebook 1) y lo transforma en el formato de secuencias necesario para entrenar un modelo LSTM.\n",
    "\n",
    "**Pasos Realizados:**\n",
    "\n",
    "1.  **Carga de Datos:** Se carga el `df_final` (en formato `.csv` o `.parquet`).\n",
    "2.  **Preprocesamiento Mínimo:**\n",
    "    * **Ordenamiento:** Se asegura de que los datos estén estrictamente ordenados cronológicamente por cliente (`POLIZA_SUMINISTRO`), `FECHA` y `HORA`. **Esto es crucial** para la correcta creación de secuencias temporales.\n",
    "    * **Verificación de Tipos:** Realiza una comprobación final y conversión (si es necesario) de las columnas numéricas (clima, consumo) a `float`, la columna `FESTIVO` a `int`, y aplica *one-hot encoding* a `TIPO_DIA` si no se hizo previamente.\n",
    "3.  **Definición de Features y Target:**\n",
    "    * Se definen las columnas que actuarán como **features (`X`)** para el modelo. Estas incluyen `CONSUMO_REAL`, las variables climáticas (`TEMP_MEDIA`, `TEMP_MIN`, `TEMP_MAX`, `PRECIPITACION`, `HUMEDAD_RELATIVA_MEDIA`), `FESTIVO`, y las columnas generadas por el *one-hot encoding* de `TIPO_DIA`. Se excluyen IDs y columnas de fecha/hora.\n",
    "    * Se define la columna objetivo (`y`) como `FUGA_DETECTADA`.\n",
    "4.  **Manejo Final de NaNs:** Se realiza una última verificación de valores nulos en las *features* y el *target*. Si se encuentran, se aplica un relleno (`ffill` y `bfill`) **agrupado por cliente** para evitar rellenar con datos de otros clientes. Las filas con NaNs persistentes se eliminan.\n",
    "5.  **Creación de Secuencias (Ventana Deslizante):**\n",
    "    * Se agrupan los datos por `POLIZA_SUMINISTRO`.\n",
    "    * Para cada cliente, se desliza una \"ventana\" de tamaño `N_PAST_STEPS` (ej. 72 horas/registros) a través de sus datos.\n",
    "    * Cada ventana de *features* se convierte en una muestra para el array `X`.\n",
    "    * El valor de `FUGA_DETECTADA` inmediatamente posterior a cada ventana se toma como el objetivo para el array `y`.\n",
    "    * Este proceso garantiza que las secuencias solo contengan datos de un único cliente y en orden cronológico.\n",
    "6.  **Guardado de Datos:**\n",
    "    * Los arrays resultantes `X` (forma: `[número_de_secuencias, N_PAST_STEPS, número_de_features]`) e `y` (forma: `[número_de_secuencias]`) se guardan como archivos `.npy` **sin escalar**.\n",
    "    * **Importante:** El escalado de *features* (ej., `MinMaxScaler`) se realizará en el siguiente notebook (`03_Model_Training.ipynb`) **después** de dividir los datos en conjuntos de entrenamiento, validación y prueba para evitar fuga de datos (*data leakage*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe7226bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00557f6",
   "metadata": {},
   "source": [
    "## 1.1. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e5a3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final_dataset\n",
    "df_final = pd.read_csv('../data/final_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55dfbc",
   "metadata": {},
   "source": [
    "## 1.2. Preprocesamiento mínimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d7553a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Minimal Preprocessing ---\n",
      "Sorted DataFrame by POLIZA_SUMINISTRO, FECHA, and HORA.\n",
      "Converting 'TEMP_MEDIA' back to numeric (cleaning just in case)...\n",
      "Converting 'TEMP_MIN' back to numeric (cleaning just in case)...\n",
      "Converting 'TEMP_MAX' back to numeric (cleaning just in case)...\n",
      "Converting 'PRECIPITACION' back to numeric (cleaning just in case)...\n",
      "Converted 'FESTIVO' to integer.\n",
      "One-hot encoding 'TIPO_DIA'...\n",
      "Encoded 'TIPO_DIA' into: ['TIPO_DIA_Laborable', 'TIPO_DIA_Fin de Semana']\n"
     ]
    }
   ],
   "source": [
    "# Minimal preprocessing to ensure correct types and sorting\n",
    "print(\"\\n--- Applying Minimal Preprocessing ---\")\n",
    "\n",
    "# Ensure correct sorting \n",
    "# Convert 'HORA' to string if not already\n",
    "df_final['HORA'] = df_final['HORA'].astype(str)\n",
    "df_final = df_final.sort_values(by=['POLIZA_SUMINISTRO', 'FECHA', 'HORA'])\n",
    "print(\"Sorted DataFrame by POLIZA_SUMINISTRO, FECHA, and HORA.\")\n",
    "\n",
    "# Convert numeric columns from object to float if necessary\n",
    "numeric_cols = ['TEMP_MEDIA', 'TEMP_MIN', 'TEMP_MAX', 'PRECIPITACION', 'HUMEDAD_RELATIVA_MEDIA', 'CONSUMO_REAL']\n",
    "for col in numeric_cols:\n",
    "    if col in df_final.columns and df_final[col].dtype == 'object':\n",
    "        print(f\"Converting '{col}' back to numeric (cleaning just in case)...\")\n",
    "        df_final[col] = df_final[col].astype(str).str.replace(',', '.', regex=False)\n",
    "        df_final[col] = df_final[col].str.extract(r'([+-]?\\d+(?:\\.\\d*)?|[+-]?\\.\\d+)', expand=False)\n",
    "        df_final[col] = pd.to_numeric(df_final[col], errors='coerce')\n",
    "    elif col in df_final.columns:\n",
    "         # Ensure it's float\n",
    "         df_final[col] = pd.to_numeric(df_final[col], errors='coerce').astype(float)\n",
    "\n",
    "\n",
    "if 'FESTIVO' in df_final.columns and df_final['FESTIVO'].dtype != 'int':\n",
    "    df_final['FESTIVO'] = df_final['FESTIVO'].astype(int)\n",
    "    print(\"Converted 'FESTIVO' to integer.\")\n",
    "\n",
    "# One-hot encode 'TIPO_DIA' if it exists and is categorical\n",
    "initial_cols = set(df_final.columns)\n",
    "new_one_hot_cols = []\n",
    "if 'TIPO_DIA' in df_final.columns and df_final['TIPO_DIA'].dtype == 'object':\n",
    "    print(\"One-hot encoding 'TIPO_DIA'...\")\n",
    "    df_final['TIPO_DIA'] = df_final['TIPO_DIA'].astype('category')\n",
    "    df_final = pd.get_dummies(df_final, columns=['TIPO_DIA'], drop_first=True, dummy_na=False)\n",
    "    new_one_hot_cols = list(set(df_final.columns) - initial_cols)\n",
    "    print(f\"Encoded 'TIPO_DIA' into: {new_one_hot_cols}\")\n",
    "else:\n",
    "    # Check if already encoded\n",
    "    new_one_hot_cols = [col for col in df_final.columns if col.startswith('TIPO_DIA_')]\n",
    "    if new_one_hot_cols:\n",
    "        print(f\"'TIPO_DIA' seems already encoded into: {new_one_hot_cols}\")\n",
    "    else:\n",
    "        print(\"'TIPO_DIA' column not found or already numeric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231771a",
   "metadata": {},
   "source": [
    "## 1.3. Definición de Features y Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f22813b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using features: ['CONSUMO_REAL', 'TEMP_MEDIA', 'TEMP_MIN', 'TEMP_MAX', 'PRECIPITACION', 'HUMEDAD_RELATIVA_MEDIA', 'FESTIVO', 'TIPO_DIA_Laborable', 'TIPO_DIA_Fin de Semana']\n"
     ]
    }
   ],
   "source": [
    "# Define Features and Target \n",
    "\n",
    "N_PAST_STEPS = 72 # Number of past time steps (rows) to use as input\n",
    "N_FUTURE_STEPS = 1 # Number of future time steps to predict (typically 1 for classification)\n",
    "\n",
    "# Define feature columns\n",
    "FEATURE_COLUMNS = [\n",
    "    'CONSUMO_REAL',\n",
    "    'TEMP_MEDIA',\n",
    "    'TEMP_MIN',\n",
    "    'TEMP_MAX',\n",
    "    'PRECIPITACION',\n",
    "    'HUMEDAD_RELATIVA_MEDIA',\n",
    "    'FESTIVO',\n",
    "] + new_one_hot_cols \n",
    "\n",
    "# Filter FEATURE_COLUMNS to only those actually present in the DataFrame\n",
    "FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col in df_final.columns]\n",
    "print(f\"\\nUsing features: {FEATURE_COLUMNS}\")\n",
    "\n",
    "\n",
    "TARGET_COLUMN = 'FUGA_DETECTADA'\n",
    "if TARGET_COLUMN not in df_final.columns:\n",
    "     print(f\"ERROR: Target column '{TARGET_COLUMN}' not found.\")\n",
    "     exit()\n",
    "     \n",
    "# Ensure target column is integer\n",
    "df_final[TARGET_COLUMN] = df_final[TARGET_COLUMN].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb92630",
   "metadata": {},
   "source": [
    "## 1.4. Manejo final de NaNs (Doble verificación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef72ed40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final check for NaNs before creating sequences:\n",
      "CONSUMO_REAL              0\n",
      "TEMP_MEDIA                0\n",
      "TEMP_MIN                  0\n",
      "TEMP_MAX                  0\n",
      "PRECIPITACION             0\n",
      "HUMEDAD_RELATIVA_MEDIA    0\n",
      "FESTIVO                   0\n",
      "TIPO_DIA_Laborable        0\n",
      "TIPO_DIA_Fin de Semana    0\n",
      "FUGA_DETECTADA            0\n",
      "dtype: int64\n",
      "Final NaN handling complete.\n"
     ]
    }
   ],
   "source": [
    "# handle NaNs in features and target\n",
    "print(\"\\nFinal check for NaNs before creating sequences:\")\n",
    "print(df_final[FEATURE_COLUMNS + [TARGET_COLUMN]].isnull().sum())\n",
    "if df_final[FEATURE_COLUMNS + [TARGET_COLUMN]].isnull().values.any():\n",
    "    print(\"Applying final ffill and bfill for NaNs...\")\n",
    "    # Group by customer is essential here\n",
    "    df_final[FEATURE_COLUMNS] = df_final.groupby('POLIZA_SUMINISTRO')[FEATURE_COLUMNS].ffill()\n",
    "    df_final[FEATURE_COLUMNS] = df_final.groupby('POLIZA_SUMINISTRO')[FEATURE_COLUMNS].bfill()\n",
    "    initial_rows = len(df_final)\n",
    "    # Drop rows where NaNs couldn't be filled (e.g., beginning of a customer's data)\n",
    "    df_final.dropna(subset=FEATURE_COLUMNS + [TARGET_COLUMN], inplace=True)\n",
    "    if len(df_final) < initial_rows:\n",
    "        print(f\"Dropped {initial_rows - len(df_final)} rows with persistent NaNs.\")\n",
    "print(\"Final NaN handling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6489dc2b",
   "metadata": {},
   "source": [
    "## 1.5. Creación de secuencias (ventana deslizante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6fdd140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Implementing Sliding Window ---\n",
      "Creating sequences with N_PAST_STEPS=72...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 68.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequences:\n",
      "X shape: (121114, 72, 9)\n",
      "y shape: (121114,)\n"
     ]
    }
   ],
   "source": [
    "# Implement Sliding Window per Customer \n",
    "print(\"\\n--- Implementing Sliding Window ---\")\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "grouped = df_final.groupby('POLIZA_SUMINISTRO')\n",
    "print(f\"Creating sequences with N_PAST_STEPS={N_PAST_STEPS}...\")\n",
    "\n",
    "for customer_id, group in tqdm(grouped):\n",
    "    # Select feature columns and target column as NumPy arrays\n",
    "    features = group[FEATURE_COLUMNS].values\n",
    "    target = group[TARGET_COLUMN].values\n",
    "\n",
    "    # Ensure there's enough data for at least one sequence + target\n",
    "    if len(group) >= N_PAST_STEPS + N_FUTURE_STEPS:\n",
    "        # Iterate to create sequences\n",
    "        for i in range(len(group) - N_PAST_STEPS - N_FUTURE_STEPS + 1):\n",
    "            X_sequences.append(features[i : i + N_PAST_STEPS])\n",
    "            # Target is the value N_FUTURE_STEPS after the sequence ends\n",
    "            y_targets.append(target[i + N_PAST_STEPS + N_FUTURE_STEPS - 1]) # Adjust index based on N_FUTURE_STEPS\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X_sequences)\n",
    "y = np.array(y_targets)\n",
    "\n",
    "print(f\"\\nGenerated sequences:\")\n",
    "if X.size > 0 and y.size > 0:\n",
    "    print(f\"X shape: {X.shape}\") # (num_samples, N_PAST_STEPS, num_features)\n",
    "    print(f\"y shape: {y.shape}\") # (num_samples,)\n",
    "else:\n",
    "    print(\"WARNING: No sequences were generated. Check data length and parameters.\")\n",
    "    exit() # Exit if no sequences could be created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b318f6b",
   "metadata": {},
   "source": [
    "## 1.6. Guardar datos de secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f100151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Prepared Data ---\n",
      "\n",
      "UNSCALED prepared data saved successfully:\n",
      "- Sequences saved to: ../data/X_sequences_unscaled.npy\n",
      "- Targets saved to: ../data/y_targets.npy\n"
     ]
    }
   ],
   "source": [
    "# Save UN-SCALED Data into data directory\n",
    "print(\"\\n--- Saving Prepared Data ---\")\n",
    "\n",
    "# Define the target directory relative to the notebook's location\n",
    "data_folder = '../data/'\n",
    "\n",
    "# Create the directory if it doesn't exist (optional but good practice)\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Define full file paths using os.path.join\n",
    "x_filename = os.path.join(data_folder, 'X_sequences_unscaled.npy')\n",
    "y_filename = os.path.join(data_folder, 'y_targets.npy')\n",
    "\n",
    "try:\n",
    "    np.save(x_filename, X)\n",
    "    np.save(y_filename, y)\n",
    "    print(f\"\\nUNSCALED prepared data saved successfully:\")\n",
    "    print(f\"- Sequences saved to: {x_filename}\")\n",
    "    print(f\"- Targets saved to: {y_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR saving prepared data: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
